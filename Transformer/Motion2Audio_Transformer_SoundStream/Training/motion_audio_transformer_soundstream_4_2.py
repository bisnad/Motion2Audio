"""
motion to audio transformer that operates on the audio side on the codes generated by the SoundStream model
"""
"""
Imports
"""

import torch
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
from torch import nn
import torch.nn.functional as nnF
from collections import OrderedDict
import scipy.linalg as sclinalg

import math
import os, sys, time, subprocess
import numpy as np
import csv
import matplotlib.pyplot as plt
import tqdm

# audio specific imports

import torchaudio
import torchaudio.transforms as transforms
import simpleaudio as sa
import auraloss

# soundstream specific imports
from soundstream import from_pretrained as soundstream_from_pretrained

# mocap specific imports

from common import utils
from common import bvh_tools as bvh
from common import fbx_tools as fbx
from common import mocap_tools as mocap
from common.quaternion import qmul, qrot, qnormalize_np, slerp
from common.pose_renderer import PoseRenderer

"""
Compute Device
"""

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print('Using {} device'.format(device))

"""
Mocap Settings
"""

"""
mocap_data_path = "E:/data/mocap/Diane/Solos/ZHdK_10.10.2025/fbx_50hz/"
#mocap_data_files = ["trial-001.fbx"]
#mocap_valid_ranges = [[466, 18947]]
mocap_data_files = ["trial-002.fbx"]
mocap_valid_ranges = [[257, 18951]]
"""

"""
mocap_data_path = "E:/data/mocap/Motion2Audio/stocos/fbx_50hz/"
mocap_data_files = ["Take_3_50fps_crop.fbx"]
mocap_valid_ranges = [[0, 12350]]
"""


mocap_data_path = "E:/data/mocap/Eleni/Solos/ZHdK_04.12.2025/fbx_50hz/"
mocap_data_files = ["Eline_Session-002.fbx"]
mocap_valid_ranges = [[644, 41796]]


"""
mocap_data_path = "E:/Data/mocap/Diane/Solos/ZHdK_12.11.2025/fbx_50hz/RepeatedExcerpt/"
mocap_data_files = ["Diane_Take1.fbx",
                    "Diane_Take2.fbx",
                    "Diane_Take3.fbx",
                    "Diane_Take4.fbx"]
mocap_valid_ranges = [[530, 3544],
                      [346, 3366],
                      [351, 3365],
                      [244, 3258]]
"""


mocap_pos_scale = 0.1
mocap_fps = 50
mocap_dim = -1 # automatically calculated
mocap_input_seq_length = 60
mocap_output_seq_length = 10 # for non teacher forcing
mocap_input_output_seq_length = mocap_input_seq_length + mocap_output_seq_length
load_mocap_stat = False
mocap_mean_file = "results/stat/mocap_mean.pt"
mocap_std_file = "results/stat/mocap_std.pt"

"""
Audio Settings
"""

"""
audio_data_path = "E:/data/audio/Diane/48khz/"
audio_data_files = ["4d69949b.wav"]
#audio_valid_ranges = [[5.0, 374.46]]
audio_valid_ranges = [[0.28, 374.23]]
"""

"""
audio_data_path = "E:/data/audio/Motion2Audio/stocos/"
audio_data_files = ["Take3_RO_37-4-1_HQ_audio_crop_48khz.wav"]
audio_valid_ranges = [[-1.0, -1.0]]
"""

audio_data_path = "E:/data/audio/Eleni/"
audio_data_files = ["4_5870821179501060412.wav"]
audio_valid_ranges = [[2.71, 825.75]]

"""
audio_data_path = "E:/data/audio/Diane/48khz/"
audio_data_files = ["4d69949b_Excerpt.wav",
                    "4d69949b_Excerpt.wav",
                    "4d69949b_Excerpt.wav",
                    "4d69949b_Excerpt.wav"]
audio_valid_ranges = [[0.0, 60.28],
                      [0.0, 60.28],
                      [0.0, 60.28],
                      [0.0, 60.28]]
"""

audio_orig_sample_rate = 48000
audio_channels = 1
audio_dim = -1 # automatically calculated
audio_waveform_input_seq_length = int(audio_orig_sample_rate / mocap_fps * mocap_input_seq_length)
audio_samples_per_mocap_frame = audio_orig_sample_rate // mocap_fps

load_audio_soundstream_stat = False
audio_soundstream_mean_file = "results/stat/soundstream_mean.pt"
audio_soundstream_std_file = "results/stat/soundstream_std.pt"

"""
SoundStream Settings
"""

soundstream_norm_path = "results_soundstream_dim32/soundstream_q_norm.pt"
soundstream_audio_sample_rate = 16000
min_soundstream_samples = 16000

"""
Dataset Settings
"""

mocap_frame_incr = 1
batch_size = 32 # 128
test_percentage = 0.1
load_dataset = True

"""
Transformer Model Settings
"""

transformer_layer_count = 6
transformer_head_count = 8
transformer_embed_dim = 256
transformer_dropout = 0.1   
pos_encoding_max_length = None # will be calculated

"""
Training Settings
"""

learning_rate = 1e-4
non_teacher_forcing_step_count = 10
model_save_interval = 50
load_weights = True
save_weights = False
transformer_load_weights_path = "results/weights/transformer_weights_epoch_200"
epochs = 200


"""
Mocap Visualisation Settings
"""

view_ele = 90.0
view_azi = -90.0
view_line_width = 1.0
view_size = 4.0

"""
Load Data - Mocap
"""

# load mocap data
bvh_tools = bvh.BVH_Tools()
fbx_tools = fbx.FBX_Tools()
mocap_tools = mocap.Mocap_Tools()

mocap_all_data = []

for mocap_data_file, mocap_valid_range in zip(mocap_data_files, mocap_valid_ranges):

    if mocap_data_file.endswith(".bvh") or mocap_data_file.endswith(".BVH"):
        bvh_data = bvh_tools.load(mocap_data_path + "/" + mocap_data_file)
        mocap_data = mocap_tools.bvh_to_mocap(bvh_data)
    elif mocap_data_file.endswith(".fbx") or mocap_data_file.endswith(".FBX"):
        fbx_data = fbx_tools.load(mocap_data_path + "/" + mocap_data_file)
        mocap_data = mocap_tools.fbx_to_mocap(fbx_data)[0] # first skeleton only  
        
    mocap_data["skeleton"]["offsets"] *= mocap_pos_scale
    mocap_data["motion"]["pos_local"] *= mocap_pos_scale
    
    #print("pos_local shape", mocap_data["motion"]["pos_local"].shape)
    #print("rot_local_euler shape", mocap_data["motion"]["rot_local_euler"].shape)
    
    mocap_data["motion"]["pos_local"] = mocap_data["motion"]["pos_local"][mocap_valid_range[0]:mocap_valid_range[1], ...]
    mocap_data["motion"]["rot_local_euler"] = mocap_data["motion"]["rot_local_euler"][mocap_valid_range[0]:mocap_valid_range[1], ...]
    
    # set x and z offset of root joint to zero
    mocap_data["skeleton"]["offsets"][0, 0] = 0.0 
    mocap_data["skeleton"]["offsets"][0, 2] = 0.0 
    
    # set x and z offset of root joint to zero
    mocap_data["skeleton"]["offsets"][0, 0] = 0.0 
    mocap_data["skeleton"]["offsets"][0, 2] = 0.0
    
    if mocap_data_file.endswith(".bvh") or mocap_data_file.endswith(".BVH"):
        mocap_data["motion"]["rot_local"] = mocap_tools.euler_to_quat_bvh(mocap_data["motion"]["rot_local_euler"], mocap_data["rot_sequence"])
    elif mocap_data_file.endswith(".fbx") or mocap_data_file.endswith(".FBX"):
        mocap_data["motion"]["rot_local"] = mocap_tools.euler_to_quat(mocap_data["motion"]["rot_local_euler"], mocap_data["rot_sequence"])

    mocap_all_data.append(mocap_data)
    
# get mocap info

mocap_skeleton = mocap_all_data[0]["skeleton"]

offsets = mocap_skeleton["offsets"].astype(np.float32)
parents = mocap_skeleton["parents"]
children = mocap_skeleton["children"]

mocap_motion = mocap_all_data[0]["motion"]["rot_local"]

joint_count = mocap_motion.shape[1]
joint_dim = mocap_motion.shape[2]
pose_dim = joint_count * joint_dim
mocap_dim = pose_dim

# create edge list
def get_edge_list(children):
    edge_list = []

    for parent_joint_index in range(len(children)):
        for child_joint_index in children[parent_joint_index]:
            edge_list.append([parent_joint_index, child_joint_index])
    
    return edge_list

edge_list = get_edge_list(children)

poseRenderer = PoseRenderer(edge_list)
    
# calc mean and std on all mocap data

if load_mocap_stat == True:
    mocap_mean = torch.load(mocap_mean_file)
    mocap_std = torch.load(mocap_std_file)
    
    mocap_mean.to(device)
    mocap_std.to(device)
else:

    mocap_sequences_concat = [ mocap_data["motion"]["rot_local"] for mocap_data in mocap_all_data ]
    mocap_sequences_concat = np.concatenate(mocap_sequences_concat, axis=0)
    mocap_sequences_concat = mocap_sequences_concat.reshape(mocap_sequences_concat.shape[0], -1)
    
    mocap_mean = np.mean(mocap_sequences_concat, axis=0, keepdims=True)
    mocap_std = np.std(mocap_sequences_concat, axis=0, keepdims=True)
    
    mocap_mean = torch.from_numpy(mocap_mean).to(dtype=torch.float32)
    mocap_std = torch.from_numpy(mocap_std).to(dtype=torch.float32)
    
    print("mocap_mean s ", mocap_mean.shape)
    print("mocap_std s ", mocap_std.shape)
    
    torch.save(mocap_mean, mocap_mean_file)
    torch.save(mocap_std, mocap_std_file)
    
    mocap_mean = mocap_mean.to(device)
    mocap_std = mocap_std.to(device)

"""
Load Data - Audio
"""

audio_all_data = []

for audio_data_file, audio_valid_range in zip(audio_data_files, audio_valid_ranges):   
    
    audio_data, _ = torchaudio.load(audio_data_path + audio_data_file)
  
    #print("audio_data s ", audio_data.shape)
    
    audio_range_begin = audio_valid_range[0]
    audio_range_end = audio_valid_range[1]
    
    if audio_range_begin > 0 and audio_range_end > 0:
        audio_valid_range_sample = [ int(audio_valid_range[0] * audio_orig_sample_rate), int(audio_valid_range[1] * audio_orig_sample_rate)]    
    else: 
        audio_valid_range_sample = [ 0, audio_data.shape[-1]]   

    #print("audio_valid_range_sample ", audio_valid_range_sample)
    
    audio_data = audio_data[:, audio_valid_range_sample[0]:audio_valid_range_sample[1]]
    
    #print("audio_data 2 s ", audio_data.shape)
    
    audio_all_data.append(audio_data)

"""
Create SoundStream Model
"""

class SoundStreamSpecBackend:
    """
    SoundStream wrapper:

    wav_to_spec: [B,T_48k] -> [B,F,T_q] (normalized codes)
    spec_to_wav: [B,F,T_q] (or [B,1,F,T_q]) -> [B,T_16k]
    """

    def __init__(
        self,
        device: torch.device,
        audio_sample_rate: int = 48000,
        target_sr: int = 16000,
        min_len_16k: int = 16000,
        q_mean: float | None = None,
        q_std: float | None = None,
    ):
        assert soundstream_from_pretrained is not None, \
            "pip install soundstream to use SoundStreamSpecBackend."

        self.device = device
        self.audio_sample_rate = audio_sample_rate
        self.orig_sample_rate = audio_sample_rate
        self.target_sr = target_sr
        self.min_len_16k = min_len_16k

        print("Loading SoundStream (NaturalSpeech2 config)...")
        self.model = soundstream_from_pretrained().to(device)
        self.model.eval()
        for p in self.model.parameters():
            p.requires_grad_(False)

        self.sample_rate = self.target_sr

        # Normalization stats (must come from dataset-level computation)
        if (q_mean is not None) and (q_std is not None):
            self._q_mean = torch.tensor(q_mean, device=self.device, dtype=torch.float32)
            self._q_std = torch.tensor(q_std, device=self.device, dtype=torch.float32).clamp_min(1e-3)
            print(f"[SoundStreamSpecBackend] Loaded q_mean={float(self._q_mean):.6f}, q_std={float(self._q_std):.6f}")
        else:
            self._q_mean = None
            self._q_std = None
            print("[SoundStreamSpecBackend] No normalization stats set yet.")
    def set_norm_stats(self, q_mean: float, q_std: float):
        """
        Set global normalization stats after computing them on the dataset.
        """
        self._q_mean = torch.tensor(q_mean, device=self.device, dtype=torch.float32)
        self._q_std = torch.tensor(q_std, device=self.device, dtype=torch.float32).clamp_min(1e-3)
        print(f"[SoundStreamSpecBackend] Normalization stats updated: "
              f"q_mean={float(self._q_mean):.6f}, q_std={float(self._q_std):.6f}")
    @torch.no_grad()
    def _encode(self, wav_16k: torch.Tensor) -> torch.Tensor:
        """
        wav_16k: [B,1,T_16k]
        returns code-like tensor [B,F,T_q]
        """
        quantized = self.model(wav_16k, mode="encode")
        if isinstance(quantized, torch.Tensor):
            q = quantized
        elif isinstance(quantized, (list, tuple)):
            q_tensors = [
                qt if isinstance(qt, torch.Tensor) else torch.as_tensor(qt)
                for qt in quantized
            ]
            q = torch.cat(q_tensors, dim=1)
        else:
            raise RuntimeError(f"Unexpected quantized type: {type(quantized)}")

        if q.dim() == 2:
            q = q.unsqueeze(1)
        elif q.dim() == 3:
            pass
        else:
            raise RuntimeError(f"Unexpected quantized shape: {q.shape}")

        return q  # [B,F,T_q]

    @torch.no_grad()
    def wav_to_spec(self, wav_16k: torch.Tensor) -> torch.Tensor:
        """
        wav: [B,T] or [T] at audio_sample_rate
        returns: [B,F,T_q], normalized
        """
        if self._q_mean is None or self._q_std is None:
            raise RuntimeError(
                "SoundStreamSpecBackend normalization stats not set. "
                "Compute them once on the dataset and call set_norm_stats(), "
                "or pass q_mean/q_std into the constructor."
            )
            
        #print("wav_16k s ", wav_16k.shape)

        if wav_16k.dim() == 1:
            wav_16k = wav_16k.unsqueeze(0)
        wav_16k = wav_16k.to(self.device).float()
        
        #print("wav_16k 2 s ", wav_16k.shape)

        B, T_16k = wav_16k.shape
        if T_16k < self.min_len_16k:
            pad = self.min_len_16k - T_16k
            wav_16k = nnF.pad(wav_16k, (0, pad))
            
        #print("wav_16k 3 s ", wav_16k.shape)

        wav_16k = wav_16k.unsqueeze(1)  # [B,1,T_16k]
        
        #print("wav_16k 4 s ", wav_16k.shape)
        
        q = self._encode(wav_16k)       # [B,F,T_q]
        
        #print("q s ", q.shape)

        q_norm = (q - self._q_mean) / (self._q_std + 1e-8)
        
        #print("q_norm s ", q_norm.shape)
        
        return q_norm


    @torch.no_grad()
    def extract_spec(self, wav: torch.Tensor) -> torch.Tensor:
        spec = self.wav_to_spec(wav)     # [B,F,T]
        return spec.unsqueeze(1)

    @torch.no_grad()
    def spec_to_wav(self, spec: torch.Tensor) -> torch.Tensor:

        if spec.dim() == 4:
            spec = spec.squeeze(1)
        if self._q_mean is None or self._q_std is None:
            raise RuntimeError(
                "SoundStreamSpecBackend normalization stats not set. "
                "Cannot denormalize specs."
            )

        q = spec * self._q_std + self._q_mean
        q = q.to(self.device)

        wav_16k = self.model(q, mode="decode")  # [B,1,T]
        wav_16k = wav_16k.squeeze(1)
        return wav_16k

# first create sound stream backend without audio stats
soundstream_nostat = SoundStreamSpecBackend(
        device=device,
        audio_sample_rate=audio_orig_sample_rate,
        target_sr=soundstream_audio_sample_rate,
        min_len_16k=min_soundstream_samples,
    )

# calculate soundstream norm stats
class AudioWindowDataset(Dataset):
    def __init__(self, audio_tensors: list[torch.Tensor],
                 window_length: int, hop_length: int,
                 max_windows_per_clip: int | None = None):
        self.window_length = window_length
        self.windows = []
        for w in audio_tensors:
            w = w.squeeze(0)  # [T]
            T = w.shape[0]
            # include last valid start index as well
            starts = list(range(0, max(0, T - window_length + 1), hop_length))
            if max_windows_per_clip is not None:
                starts = starts[:max_windows_per_clip]
            for s in starts:
                self.windows.append((w, s))
        print(f"[AudioWindowDataset] Total windows: {len(self.windows)}")

    def __len__(self):
        return len(self.windows)

    def __getitem__(self, idx):
        w, s = self.windows[idx]
        win = w[s:s + self.window_length]
        # pad last window if slightly short
        if win.shape[0] < self.window_length:
            pad = self.window_length - win.shape[0]
            win = nnF.pad(win, (0, pad))
        return win

def compute_soundstream_norm_stats(
    backend: SoundStreamSpecBackend,
    audio_all_data: list[torch.Tensor],
    window_length: int,
    hop_length: int,
    max_windows_per_clip: int | None = None,
    batch_size: int = 8,
    stats_path: str | None = None,
) -> tuple[float, float]:
    """
    Compute global mean/std of SoundStream codes q over the dataset (before normalization).

    Returns:
        (q_mean, q_std) as floats. Optionally saves them to stats_path.
    """
    # Build same window dataset as training
    dataset = AudioWindowDataset(
        audio_tensors=audio_all_data,
        window_length=window_length,
        hop_length=hop_length,
        max_windows_per_clip=max_windows_per_clip,
    )
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=False)

    print("[compute_soundstream_norm_stats] Computing global q_mean/q_std...")
    total_sum = 0.0
    total_sq = 0.0
    total_count = 0

    device = backend.device
    for batch_wav in tqdm.tqdm(loader, desc="[SS stats]"):
        batch_wav = batch_wav.to(device).float()  # [B, 48000] at 48k

        # --- same preprocessing as wav_to_spec, but without normalization ---
        if backend.audio_sample_rate != backend.target_sr:
            wav_16k = torchaudio.functional.resample(
                batch_wav, backend.audio_sample_rate, backend.target_sr
            )
        else:
            wav_16k = batch_wav

        B, T_16k = wav_16k.shape
        if T_16k < backend.min_len_16k:
            pad = backend.min_len_16k - T_16k
            wav_16k = nnF.pad(wav_16k, (0, pad))

        wav_16k = wav_16k.unsqueeze(1)  # [B,1,T_16k]
        q = backend._encode(wav_16k)    # [B,F,T_q]

        q_flat = q.view(-1).float()
        total_sum += float(q_flat.sum().item())
        total_sq += float((q_flat * q_flat).sum().item())
        total_count += q_flat.numel()

    if total_count == 0:
        raise RuntimeError("No SoundStream codes found when computing stats.")

    q_mean = total_sum / total_count
    q_var = max(total_sq / total_count - q_mean * q_mean, 1e-6)
    q_std = float(np.sqrt(q_var))

    print(f"[compute_soundstream_norm_stats] q_mean={q_mean:.6f}, q_std={q_std:.6f}, count={total_count}")

    if stats_path is not None:
        os.makedirs(os.path.dirname(stats_path), exist_ok=True)
        torch.save({"q_mean": q_mean, "q_std": q_std}, stats_path)
        print(f"[compute_soundstream_norm_stats] Saved stats to {stats_path}")

    backend.set_norm_stats(q_mean, q_std)
    return q_mean, q_std


q_mean, q_std = compute_soundstream_norm_stats(soundstream_nostat, 
                                               audio_all_data, 
                                               audio_orig_sample_rate, 
                                               audio_samples_per_mocap_frame, 
                                               2000)

# create sound stream backend with audio stats

soundstream = SoundStreamSpecBackend(
        device=device,
        audio_sample_rate=audio_orig_sample_rate,      # uses your global config
        target_sr=soundstream_audio_sample_rate,
        min_len_16k=min_soundstream_samples,
        q_mean=q_mean ,
        q_std=q_std,
    )

audio_all_data[0].shape
audio_all_data[0].dtype

dummy_excerpt_time = 10
audio_waveform_input_sequence = audio_all_data[0][:, dummy_excerpt_time * audio_orig_sample_rate : dummy_excerpt_time * audio_orig_sample_rate + audio_waveform_input_seq_length]
audio_waveform_input_sequence = torchaudio.functional.resample(audio_waveform_input_sequence, audio_orig_sample_rate, soundstream_audio_sample_rate)

audio_soundstream_input_sequence = soundstream.wav_to_spec(audio_waveform_input_sequence.to(device))
audio_soundstream_input_seq_length = audio_soundstream_input_sequence.shape[1]
audio_dim = audio_soundstream_input_sequence.shape[-1]

print("audio_soundstream_input_seq_length ", audio_soundstream_input_seq_length)
print("audio_dim ", audio_dim)

pos_encoding_max_length = max(mocap_input_seq_length, audio_soundstream_input_seq_length)

rec_audio_waveform = soundstream.spec_to_wav(audio_soundstream_input_sequence)

print("audio_waveform_input_sequence s ", audio_waveform_input_sequence.shape)
print("audio_soundstream_input_sequence s ", audio_soundstream_input_sequence.shape)
print("rec_audio_waveform s ", rec_audio_waveform.shape)

if (audio_waveform_input_sequence.shape[-1] != rec_audio_waveform.shape[-1]):
    print("ISSUE: the length of the audio waveform differs between input and output!!!!!")

"""
sa.play_buffer(audio_waveform_input_sequence.numpy(), 1, 4, soundstream_audio_sample_rate)
sa.play_buffer(rec_audio_waveform.detach().cpu().numpy(), 1, 4, soundstream_audio_sample_rate)
"""

"""
for pose_count in range(8, 256):
    
    sample_count = pose_count * audio_samples_per_mocap_frame
    
    dummy_wave = torch.zeros((1, sample_count)).to(device)
    dummy_wave = torchaudio.functional.resample(dummy_wave, audio_orig_sample_rate, soundstream_audio_sample_rate)
    dummy_soundstream = soundstream.wav_to_spec(dummy_wave.to(device))
    dummy_wave2 = soundstream.spec_to_wav(dummy_soundstream)
    
    if dummy_wave.shape[-1] == dummy_wave2.shape[-1]:
        print("pc ", pose_count)
        print("dummy_wave s ", dummy_wave.shape)
        print("dummy_soundstream s ", dummy_soundstream.shape)
"""

"""
compute and normalise audio soundstream specs
"""

if load_audio_soundstream_stat == True:
    audio_soundstream_mean = torch.load(audio_soundstream_mean_file)
    audio_soundstream_std = torch.load(audio_soundstream_std_file)
    
    audio_soundstream_mean = audio_soundstream_mean.to(device)
    audio_soundstream_std = audio_soundstream_std.to(device)
    
else:
    
    with torch.no_grad():
        
        audio_specs_all = []
        
        for audio_waveform_data in audio_all_data: 
            
            audio_waveform = torchaudio.functional.resample(audio_waveform_data, audio_orig_sample_rate, soundstream_audio_sample_rate)
            audio_spec = soundstream.wav_to_spec(audio_waveform.to(device))
            
            print("audio_spec s ", audio_spec.shape)
            
            audio_spec = audio_spec.squeeze(0)
            
            print("audio_spec 2 s ", audio_spec.shape)
            
            audio_specs_all.append(audio_spec)
                
        audio_specs_all = torch.cat(audio_specs_all, dim=0)
        
        print("audio_specs_all s ", audio_specs_all.shape)
        
        audio_soundstream_mean = torch.mean(audio_specs_all, dim=0, keepdim=True)
        audio_soundstream_std = torch.std(audio_specs_all, dim=0, keepdim=True)
        
        print("audio_soundstream_mean s ", audio_soundstream_mean.shape)
        print("audio_soundstream_std s ", audio_soundstream_std.shape)
        
        audio_soundstream_mean = audio_soundstream_mean.unsqueeze(0)
        audio_soundstream_std = audio_soundstream_std.unsqueeze(0)
        
        print("audio_soundstream_mean 2 s ", audio_soundstream_mean.shape)
        print("audio_soundstream_std 2 s ", audio_soundstream_std.shape)
        
        torch.save(audio_soundstream_mean.detach(), audio_soundstream_mean_file)
        torch.save(audio_soundstream_std.detach(), audio_soundstream_std_file)
        
        audio_soundstream_mean = audio_soundstream_mean.to(device)
        audio_soundstream_std = audio_soundstream_std.to(device)


"""
Create Dataset
"""

X_mocap = []
X_audio = []
Y_audio = []

if load_dataset == True:
    
    X_mocap = torch.load("results/dataset/X_mocap")
    X_audio = torch.load("results/dataset/X_audio")
    Y_audio = torch.load("results/dataset/Y_audio")
    
else:

    for sI in range(len(mocap_all_data)):
        
        with torch.no_grad():
            
            mocap_data = mocap_all_data[sI]["motion"]["rot_local"].reshape(-1, pose_dim)
            audio_data = audio_all_data[sI][0]
        
            #print(sI)
            #print("mocap_data s ", mocap_data.shape)
            #print("audio_data s ", audio_data.shape)
            
            mocap_frame_count = mocap_data.shape[0]
            
            # the range begin of 3 enshures no negative audio_waveform_start index and the -10 for the range end ensures that the audio_waveform_end doesn't exceed the audio waveform length
            for mfI in range(3, mocap_frame_count - mocap_input_output_seq_length - 10, mocap_frame_incr):
                
                print("mfI ", mfI, " out of ", (mocap_frame_count - mocap_input_output_seq_length - 10))
                
                # mocap sequence part
                
                # get mocap sequence
                mocap_excerpt_start = mfI
                mocap_excerpt_end = mfI + mocap_input_output_seq_length
                
                #print("mocap_excerpt_start ", mocap_excerpt_start)
                #print("mocap_excerpt_end ", mocap_excerpt_end)
                
                mocap_excerpt = mocap_data[mocap_excerpt_start:mocap_excerpt_end, :]
                mocap_excerpt = torch.from_numpy(mocap_excerpt).unsqueeze(0).to(torch.float32).to(device)
                
                #print("mfI ", mfI, " me s ", mocap_excerpt.shape)
                
                # normalise mocap excerpt
                mocap_excerpt_norm = (mocap_excerpt - mocap_mean) / (mocap_std + 1e-8) 
                
                X_mocap.append(mocap_excerpt_norm.cpu())
                
                # audio sequence part (input squence)
                asI = mfI * audio_samples_per_mocap_frame
                audio_waveform_start = asI
                audio_waveform_end = audio_waveform_start + audio_waveform_input_seq_length
                
                audio_waveform_excerpt = audio_data[audio_waveform_start:audio_waveform_end].unsqueeze(0).to(device)
                
                #print("mfI ", mfI, " audio_waveform_excerpt s ", audio_waveform_excerpt.shape)
                
                audio_waveform_excerpt = torchaudio.functional.resample(audio_waveform_excerpt, audio_orig_sample_rate, soundstream_audio_sample_rate)
                
                #print("mfI ", mfI, " audio_waveform_excerpt 2 s ", audio_waveform_excerpt.shape)
                
                audio_specs_excerpt = soundstream.wav_to_spec(audio_waveform_excerpt.to(device))
                
                #print("mfI ", mfI, " audio_specs_excerpt s ", audio_specs_excerpt.shape)
                
                audio_specs_excerpt_norm = (audio_specs_excerpt - audio_soundstream_mean) / (audio_soundstream_std + 1e-8)
                
                #print("mfI ", mfI, " audio_specs_excerpt_norm s ", audio_specs_excerpt_norm.shape)
              
                X_audio.append(audio_specs_excerpt_norm.cpu())
        
                # audio sequence part (target squence)
                
                y_audio_grouped = []
                
                for tsI in range(mocap_output_seq_length):
                    
                    mfI2 = mfI + tsI + 1
                    
                    #print("mfI2 ", mfI2)
                    
                    asI = mfI2 * audio_samples_per_mocap_frame
                    audio_waveform_start = asI
                    audio_waveform_end = audio_waveform_start + audio_waveform_input_seq_length
                    
                    audio_waveform_excerpt = audio_data[audio_waveform_start:audio_waveform_end].unsqueeze(0).to(device)
                    
                    #print("mfI2 ", mfI2, " audio_waveform_excerpt s ", audio_waveform_excerpt.shape)
                    
                    audio_waveform_excerpt = torchaudio.functional.resample(audio_waveform_excerpt, audio_orig_sample_rate, soundstream_audio_sample_rate)
                    
                    #print("mfI2 ", mfI2, " audio_waveform_excerpt 2 s ", audio_waveform_excerpt.shape)
                    
                    audio_specs_excerpt = soundstream.wav_to_spec(audio_waveform_excerpt.to(device))
                    
                    #print("mfI2 ", mfI2, " audio_specs_excerpt s ", audio_specs_excerpt.shape)
                    
                    audio_specs_excerpt_norm = (audio_specs_excerpt - audio_soundstream_mean) / (audio_soundstream_std + 1e-8)
                    
                    #print("mfI2 ", mfI2, " audio_specs_excerpt_norm s ", audio_specs_excerpt_norm.shape)
    
                    y_audio_grouped.append(audio_specs_excerpt_norm.cpu())
                    
                y_audio_grouped = torch.cat(y_audio_grouped, dim=0)
                
                #print("y_audio_grouped s ", y_audio_grouped.shape)
                
                y_audio_grouped = y_audio_grouped.unsqueeze(0)
                
                #print("y_audio_grouped 2 s ", y_audio_grouped.shape)
                
                Y_audio.append(y_audio_grouped)


    X_mocap = torch.cat(X_mocap, dim=0)
    X_audio = torch.cat(X_audio, dim=0)
    Y_audio = torch.cat(Y_audio, dim=0)
    
    torch.save(X_mocap, "results/dataset/X_mocap")
    torch.save(X_audio, "results/dataset/X_audio")
    torch.save(Y_audio, "results/dataset/Y_audio")


print("X_mocap s ", X_mocap.shape)
print("X_audio s ", X_audio.shape)
print("Y_audio s ", Y_audio.shape)


class SequenceDataset(Dataset):
    def __init__(self, X_mocap, X_audio, Y_audio):
        self.X_mocap = X_mocap
        self.X_audio = X_audio
        self.Y_audio = Y_audio
    
    def __len__(self):
        return self.X_mocap.shape[0]
    
    def __getitem__(self, idx):
        return self.X_mocap[idx, ...], self.X_audio[idx, ...], self.Y_audio[idx, ...]

full_dataset = SequenceDataset(X_mocap, X_audio, Y_audio)

x_item_mocap, x_item_audio, y_item_audio = full_dataset[0]

print("x_item_mocap s ", x_item_mocap.shape)
print("x_item_audio s ", x_item_audio.shape)
print("y_item_audio s ", y_item_audio.shape)

test_size = int(test_percentage * len(full_dataset))
train_size = len(full_dataset) - test_size

train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

x_batch_mocap, x_batch_audio, y_batch_audio = next(iter(train_loader))

print("x_batch_mocap s ", x_batch_mocap.shape)
print("x_batch_audio s ", x_batch_audio.shape)
print("y_batch_audio s ", y_batch_audio.shape)

"""
Create Models - PositionalEncoding
"""

class PositionalEncoding(nn.Module):
    def __init__(self, dim_model, dropout_p, max_len):
        super().__init__()
        self.dropout = nn.Dropout(dropout_p)

        pos_encoding = torch.zeros(max_len, dim_model)
        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1)
        division_term = torch.exp(
            torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model
        )
        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)
        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)

        # for batch-first: [1, max_len, dim_model]
        pos_encoding = pos_encoding.unsqueeze(0)
        self.register_buffer("pos_encoding", pos_encoding)
        
    def forward(self, token_embedding: torch.tensor) -> torch.tensor:
        
        #print("token_embedding s ", token_embedding.shape)
        #print("pos_encoding s ", self.pos_encoding.shape)
        
        # token_embedding: [batch_size, seq_len, dim_model]
        seq_len = token_embedding.size(1)
        # broadcast over batch dimension
        pe = self.pos_encoding[:, :seq_len, :]
        
        return self.dropout(token_embedding + pe)


"""
Create Models - Transformer
"""


class Transformer(nn.Module):

    # Constructor
    def __init__(
        self,
        mocap_dim,
        audio_dim,
        embed_dim,
        num_heads,
        num_encoder_layers,
        num_decoder_layers,
        dropout_p,
        pos_encoding_max_length
    ):
        super().__init__()

        self.embed_dim = embed_dim

        # LAYERS
        self.mocap2embed = nn.Linear(mocap_dim, embed_dim) # map mocap data to embedding
        self.audio2embed = nn.Linear(audio_dim, embed_dim) # map audio data to embedding

        self.positional_encoder = PositionalEncoding(
            dim_model=embed_dim, dropout_p=dropout_p, max_len=pos_encoding_max_length
        )
        
        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, batch_first=True)
        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers = num_encoder_layers)
        
        self.decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=num_heads, batch_first=True)
        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers = num_decoder_layers)
        
        self.embed2audio = nn.Linear(embed_dim, audio_dim) # map embedding to audio data
        
    def get_src_mask(self, size) -> torch.tensor:
        # Generates a squeare matrix where the each row allows one word more to be seen
        mask = torch.ones(size, size)
        mask = mask.float()
        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf
        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0
        
        return mask
       
    def get_tgt_mask(self, size) -> torch.tensor:
        # Generates a squeare matrix where the each row allows one word more to be seen
        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix
        mask = mask.float()
        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf
        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0
        
        # EX for size=5:
        # [[0., -inf, -inf, -inf, -inf],
        #  [0.,   0., -inf, -inf, -inf],
        #  [0.,   0.,   0., -inf, -inf],
        #  [0.,   0.,   0.,   0., -inf],
        #  [0.,   0.,   0.,   0.,   0.]]
        
        return mask
        
       
    def forward(self, mocap_data, audio_data):
        
        #print("forward")
        
        #print("data s ", data.shape)

        src_mask = self.get_src_mask(mocap_data.shape[1]).to(mocap_data.device)
        tgt_mask = self.get_tgt_mask(audio_data.shape[1]).to(audio_data.device)
        
        mocap_embedded = self.mocap2embed(mocap_data) * math.sqrt(self.embed_dim)
        mocap_embedded = self.positional_encoder(mocap_embedded)
        
        audio_embedded = self.audio2embed(audio_data) * math.sqrt(self.embed_dim)
        audio_embedded = self.positional_encoder(audio_embedded)

        # Transformer blocks - Out size = (sequence length, batch_size, num_tokens)
        encoder_out = self.encoder(mocap_embedded, mask=src_mask)
        decoder_out = self.decoder(audio_embedded, encoder_out, tgt_mask =tgt_mask)
        
        out = self.embed2audio(decoder_out)
        
        return out

transformer = Transformer(mocap_dim=mocap_dim, 
                          audio_dim=audio_dim,
                          embed_dim=transformer_embed_dim, 
                          num_heads=transformer_head_count, 
                          num_encoder_layers=transformer_layer_count, 
                          num_decoder_layers=transformer_layer_count, 
                          dropout_p=transformer_dropout,
                          pos_encoding_max_length=pos_encoding_max_length).to(device)

print(transformer)

if load_weights and transformer_load_weights_path:
    transformer.load_state_dict(torch.load(transformer_load_weights_path, map_location=device))


# test model

x_mocap_batch, x_audio_batch, y_audio_batch = next(iter(train_loader))

transformer_mocap_input = x_mocap_batch[:, :mocap_input_seq_length, ...].to(device)
transformer_audio_input = x_audio_batch.to(device)
transformer_audio_target = y_audio_batch[:, 0, ...].to(device)

transformer_audio_output = transformer(transformer_mocap_input, transformer_audio_input)

print("transformer_mocap_input s ", transformer_mocap_input.shape)
print("transformer_audio_input s ", transformer_audio_input.shape)
print("transformer_audio_target s ", transformer_audio_target.shape)
print("transformer_audio_output s ", transformer_audio_output.shape)

"""
Training
"""

optimizer = torch.optim.Adam(transformer.parameters(), lr=learning_rate)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1) # reduce the learning every 20 epochs by a factor of 10

n1_loss = nn.L1Loss()
mse_loss = nn.MSELoss()

# Define perceptial loss: MR-STFT with perceptual mel weighting
perc_loss = auraloss.freq.MultiResolutionSTFTLoss(
    fft_sizes=[1024, 2048, 8192],
    hop_sizes=[256, 512, 2048],
    win_lengths=[1024, 2048, 8192],
    scale="mel",          # use mel-scaled spectrograms
    n_bins=128,           # number of mel bins for the perceptual weighting
    sample_rate=48000,    # set to the actual SR used (48 kHz in your code)
    perceptual_weighting=True
)

def audio_specs_loss(y_specs, yhat_specs):
    
    _all = mse_loss(yhat_specs, y_specs)

    return _all

def audio_perc_loss(y_wave, yhat_wave):
    
    apl = perc_loss(yhat_wave, y_wave)
    
    return apl

def loss(y_specs_norm, yhat_specs_norm):
    
    #print("loss begin")
    
    #print("y_specs_norm s ", y_specs_norm.shape, " yhat_specs_norm s ", yhat_specs_norm.shape)
    
    #batch_size = y_specs_norm.shape[-1]
    
    #print("batch_size s ", batch_size)
    
    #y_specs_norm = y_specs_norm.reshape((-1, audio_dim))
    #yhat_specs_norm = yhat_specs_norm.reshape((-1, audio_dim))
    
    #print("y_specs_norm 2 s ", y_specs_norm.shape, " yhat_specs_norm 2 s ", yhat_specs_norm.shape)
    
    # calculate audio specs loss
    #start_time = time.time()
    
    _audio_specs_loss = audio_specs_loss(y_specs_norm, yhat_specs_norm)
    
    # calculate audio perc loss

    y_specs = y_specs_norm * audio_soundstream_std + audio_soundstream_mean
    yhat_specs = yhat_specs_norm * audio_soundstream_std + audio_soundstream_mean
    
    #print("y_specs s ", y_specs.shape, " yhat_specs s ", yhat_specs.shape)
    
    y_waveform = soundstream.spec_to_wav(y_specs)
    yhat_waveform = soundstream.spec_to_wav(yhat_specs)
    
    #print("y_waveform s ", y_waveform.shape, " yhat_waveform s ", yhat_waveform.shape)
    
    y_waveform = torchaudio.functional.resample(y_waveform, soundstream_audio_sample_rate, audio_orig_sample_rate)
    yhat_waveform = torchaudio.functional.resample(yhat_waveform, soundstream_audio_sample_rate, audio_orig_sample_rate)
    
    #print("y_waveform 2 s ", y_waveform.shape, " yhat_waveform 2 s ", yhat_waveform.shape)

    y_waveform = y_waveform.unsqueeze(1)
    yhat_waveform = yhat_waveform.unsqueeze(1)
    
    #print("y_waveform 3 s ", y_waveform.shape, " yhat_waveform 3 s ", yhat_waveform.shape)


    # calculate audio perceptual loss
    _audio_perc_loss = audio_perc_loss(y_waveform, yhat_waveform)
    
    #end_time = time.time()
    #elapsed_seconds = end_time - start_time
    #print(f"calculate audio perceptual loss: Elapsed time: {elapsed_seconds:.4f} seconds")
    
    #print("5")
    
    _total_loss = 0.0
    _total_loss += _audio_specs_loss * 0.5
    _total_loss += _audio_perc_loss * 0.5
    
    #print("loss end")
    
    return _total_loss

def train_step(x_mocap, x_audio, y_audio):

    #print("x_mocap s ", x_mocap.shape)
    #print("x_audio s ", x_audio.shape)
    #print("y_audio s ", y_audio.shape)
    
    # first step (teacher forcing)
    
    transformer_mocap_input = x_mocap[:, :mocap_input_seq_length, ...]
    transformer_audio_input = x_audio
    transformer_audio_target = y_audio[:, 0, ...]
    
    #print("transformer_mocap_input s ", transformer_mocap_input.shape)
    #print("transformer_audio_input s ", transformer_audio_input.shape)
    #print("transformer_audio_target s ", transformer_audio_target.shape)
    
    transformer_audio_output = transformer(transformer_mocap_input, transformer_audio_input)

    # todo: also compute mel loss and waveform perceptual loss
    step_loss = loss(transformer_audio_target, transformer_audio_output) 
    _loss = step_loss.detach().cpu()
    
    # Backpropagation
    optimizer.zero_grad()
    step_loss.backward()
    optimizer.step()
    
    # next steps(non-teacher forcing)
    for mfI in range(1, mocap_output_seq_length):
        
        #print("mfI ", mfI)
        
        transformer_mocap_input = x_mocap[:, mfI:mocap_input_seq_length + mfI, ...].to(device)
        transformer_audio_input = transformer_audio_output.detach().clone()
        transformer_audio_target = y_audio[:, mfI, ...]
        
        #print("transformer_mocap_input s ", transformer_mocap_input.shape)
        #print("transformer_audio_input s ", transformer_audio_input.shape)
        #print("transformer_audio_target s ", transformer_audio_target.shape)
        
        transformer_audio_output = transformer(transformer_mocap_input, transformer_audio_input)

        step_loss = loss(transformer_audio_target, transformer_audio_output) 
        _loss += step_loss.detach().cpu()

        # Backpropagation
        optimizer.zero_grad()
        step_loss.backward()
        optimizer.step()
        
    _loss /= (mocap_output_seq_length + 1)

    return _loss

"""
x_mocap_batch, x_audio_batch, y_audio_batch = next(iter(train_loader))
x_mocap_batch = x_mocap_batch.to(device)
x_audio_batch = x_audio_batch.to(device)
y_audio_batch = y_audio_batch.to(device)
_loss = train_step(x_mocap_batch, x_audio_batch, y_audio_batch)
"""

@torch.no_grad()
def test_step(x_mocap, x_audio, y_audio):
    
    transformer.eval()

    #print("x_mocap s ", x_mocap.shape)
    #print("x_audio s ", x_audio.shape)
    #print("y_audio s ", y_audio.shape)
    
    # first step (teacher forcing)
    
    transformer_mocap_input = x_mocap[:, :mocap_input_seq_length, ...]
    transformer_audio_input = x_audio
    transformer_audio_target = y_audio[:, 0, ...]
    
    #print("transformer_mocap_input s ", transformer_mocap_input.shape)
    #print("transformer_audio_input s ", transformer_audio_input.shape)
    #print("transformer_audio_target s ", transformer_audio_target.shape)
    
    transformer_audio_output = transformer(transformer_mocap_input, transformer_audio_input)

    # todo: also compute mel loss and waveform perceptual loss
    step_loss = loss(transformer_audio_target, transformer_audio_output) 
    _loss = step_loss.detach().cpu()
    
    # next steps(non-teacher forcing)
    for mfI in range(1, mocap_output_seq_length):
        
        #print("mfI ", mfI)
        
        transformer_mocap_input = x_mocap[:, mfI:mocap_input_seq_length + mfI, ...].to(device)
        transformer_audio_input = transformer_audio_output.detach().clone()
        transformer_audio_target = y_audio[:, mfI, ...]
        
        #print("transformer_mocap_input s ", transformer_mocap_input.shape)
        #print("transformer_audio_input s ", transformer_audio_input.shape)
        #print("transformer_audio_target s ", transformer_audio_target.shape)
        
        transformer_audio_output = transformer(transformer_mocap_input, transformer_audio_input)

        step_loss = loss(transformer_audio_target, transformer_audio_output) 
        _loss += step_loss.detach().cpu()

    _loss /= (mocap_output_seq_length + 1)
    
    transformer.train()

    return _loss

def train(train_dataloader, test_dataloader, epochs):
    
    loss_history = {}
    loss_history["train"] = []
    loss_history["test"] = []

    for epoch in range(epochs):
        start = time.time()
        
        _train_loss_per_epoch = []

        for train_batch in train_dataloader:
            x_mocap = train_batch[0].to(device)
            x_audio = train_batch[1].to(device)
            y_audio = train_batch[2].to(device)
            
            _loss = train_step(x_mocap, x_audio, y_audio)
            
            _loss = _loss.detach().cpu().numpy()
            
            _train_loss_per_epoch.append(_loss)

        _train_loss_per_epoch = np.mean(np.array(_train_loss_per_epoch))

        _test_loss_per_epoch = []
        
        for test_batch in test_dataloader:
            x_mocap = test_batch[0].to(device)
            x_audio = test_batch[1].to(device)
            y_audio = test_batch[2].to(device)
            
            _loss = test_step(x_mocap, x_audio, y_audio)

            _loss = _loss.detach().cpu().numpy()
            
            _test_loss_per_epoch.append(_loss)
        
        _test_loss_per_epoch = np.mean(np.array(_test_loss_per_epoch))
        
        if epoch % model_save_interval == 0 and save_weights == True:
            torch.save(transformer.state_dict(), "results/weights/transformer_weights_epoch_{}".format(epoch))
        
        loss_history["train"].append(_train_loss_per_epoch)
        loss_history["test"].append(_test_loss_per_epoch)
        
        scheduler.step()
        
        print ('epoch {} : train: {:01.4f} test: {:01.4f} time {:01.2f}'.format(epoch + 1, _train_loss_per_epoch, _test_loss_per_epoch, time.time()-start))
    
    return loss_history

# fit model
loss_history = train(train_loader, test_loader, epochs)

# save history
def save_loss_as_image(loss_history, image_file_name):
    keys = list(loss_history.keys())
    epochs = len(loss_history[keys[0]])
    
    for key in keys:
        plt.plot(range(epochs), loss_history[key], label=key)
        
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()
    plt.savefig(image_file_name)

def save_loss_as_csv(loss_history, csv_file_name):
    with open(csv_file_name, 'w') as csv_file:
        csv_columns = list(loss_history.keys())
        csv_row_count = len(loss_history[csv_columns[0]])
        
        
        csv_writer = csv.DictWriter(csv_file, fieldnames=csv_columns, delimiter=',', lineterminator='\n')
        csv_writer.writeheader()
    
        for row in range(csv_row_count):
        
            csv_row = {}
        
            for key in loss_history.keys():
                csv_row[key] = loss_history[key][row]

            csv_writer.writerow(csv_row)


save_loss_as_csv(loss_history, "results/histories/history_{}.csv".format(epochs))
save_loss_as_image(loss_history, "results/histories/history_{}.png".format(epochs))

# save model weights
torch.save(transformer.state_dict(), "results/weights/transformer_weights_epoch_{}".format(epochs))

# inference

# TODO

audio_window_length = audio_samples_per_mocap_frame * 4
audio_window_env = torch.hann_window(audio_window_length)

def forward_kinematics(rotations, root_positions):
    """
    Perform forward kinematics using the given trajectory and local rotations.
    Arguments (where N = batch size, L = sequence length, J = number of joints):
     -- rotations: (N, L, J, 4) tensor of unit quaternions describing the local rotations of each joint.
     -- root_positions: (N, L, 3) tensor describing the root joint positions.
    """

    assert len(rotations.shape) == 4
    assert rotations.shape[-1] == 4
    
    toffsets = torch.tensor(offsets).to(device)
    
    positions_world = []
    rotations_world = []

    expanded_offsets = toffsets.expand(rotations.shape[0], rotations.shape[1], offsets.shape[0], offsets.shape[1])

    # Parallelize along the batch and time dimensions
    for jI in range(offsets.shape[0]):
        if parents[jI] == -1:
            positions_world.append(root_positions)
            rotations_world.append(rotations[:, :, 0])
        else:
            positions_world.append(qrot(rotations_world[parents[jI]], expanded_offsets[:, :, jI]) \
                                   + positions_world[parents[jI]])
            if len(children[jI]) > 0:
                rotations_world.append(qmul(rotations_world[parents[jI]], rotations[:, :, jI]))
            else:
                # This joint is a terminal node -> it would be useless to compute the transformation
                rotations_world.append(None)

    return torch.stack(positions_world, dim=3).permute(0, 1, 3, 2)


def create_mocap_anim(mocap_data, mocap_start_frame_index, mocap_frame_count, file_name):
    
    pose_sequence = mocap_data[mocap_start_frame_index:mocap_start_frame_index + mocap_frame_count]

    pose_count = pose_sequence.shape[0]
    pose_sequence = np.reshape(pose_sequence, (pose_count, joint_count, joint_dim))
    
    pose_sequence = torch.tensor(np.expand_dims(pose_sequence, axis=0)).to(device)
    zero_trajectory = torch.tensor(np.zeros((1, pose_count, 3), dtype=np.float32)).to(device)
    
    skel_sequence = forward_kinematics(pose_sequence, zero_trajectory)
    
    skel_sequence = skel_sequence.detach().cpu().numpy()
    skel_sequence = np.squeeze(skel_sequence)    
    
    view_min, view_max = utils.get_equal_mix_max_positions(skel_sequence)
    skel_images = poseRenderer.create_pose_images(skel_sequence, view_min, view_max, view_ele, view_azi, view_line_width, view_size, view_size)
    skel_images[0].save(file_name, save_all=True, append_images=skel_images[1:], optimize=False, duration=1000 / mocap_fps, loop=0)
    

def create_orig_audio(waveform_data, mocap_start_frame_index, mocap_frame_count, file_name):
    
    audio_waveform_excerpt_start_index = mocap_start_frame_index * audio_samples_per_mocap_frame
    audio_waveform_excerpt_end_index = audio_waveform_excerpt_start_index + mocap_frame_count * audio_samples_per_mocap_frame

    audio_waveform_excerpt = waveform_data[audio_waveform_excerpt_start_index:audio_waveform_excerpt_end_index]
    
    torchaudio.save(file_name, audio_waveform_excerpt.unsqueeze(0), audio_orig_sample_rate)
   
create_orig_audio(audio_all_data[0][0], 60 * mocap_fps, 10 * mocap_fps, "ref_audio.wav")

@torch.no_grad()
def create_soundstream_audio(waveform_data, mocap_start_frame_index, mocap_frame_count, file_name):
    
    audio_waveform_excerpt_start_index = mocap_start_frame_index * audio_samples_per_mocap_frame
    audio_waveform_excerpt_sample_count = mocap_frame_count * audio_samples_per_mocap_frame
    audio_waveform_excerpt_end_index = audio_waveform_excerpt_start_index + audio_waveform_excerpt_sample_count

    orig_audio_waveform = waveform_data[audio_waveform_excerpt_start_index:audio_waveform_excerpt_end_index]

    gen_audio_waveform = torch.zeros((audio_waveform_excerpt_sample_count), dtype=torch.float32)
    
    #print("gen_audio_waveform s ", gen_audio_waveform.shape)
    
    for aSI in range(0, audio_waveform_excerpt_sample_count - audio_waveform_input_seq_length, audio_samples_per_mocap_frame):
        
        #print("aSI ", aSI)
        
        audio_waveform_data = orig_audio_waveform[aSI:aSI + audio_waveform_input_seq_length].to(device)
        
        #print("audio_waveform_data s ", audio_waveform_data.shape)
        
        audio_waveform = torchaudio.functional.resample(audio_waveform_data, audio_orig_sample_rate, soundstream_audio_sample_rate)
        
        #print("audio_waveform s ", audio_waveform.shape)

        audio_waveform = audio_waveform.reshape((1, -1))
        
        audio_spec = soundstream.wav_to_spec(audio_waveform.to(device))
        
        #print("audio_spec s ", audio_spec.shape)
        
        audio_waveform_2 = soundstream.spec_to_wav(audio_spec)
        
        #print("audio_waveform_2 s ", audio_waveform_2.shape)
        
        audio_waveform_2 = audio_waveform_2.flatten()
        
        #print("audio_waveform_2 2 s ", audio_waveform_2.shape)
        
        audio_waveform_data_2 = torchaudio.functional.resample(audio_waveform_2, soundstream_audio_sample_rate, audio_orig_sample_rate)
        
        #print("audio_waveform_data_2 s ", audio_waveform_data_2.shape)
        
        audio_waveform_2_window = audio_waveform_data_2.reshape(-1)[-audio_window_length:]
        audio_waveform_2_window = audio_waveform_2_window.detach().cpu()
        
        #print("audio_waveform_2_window s ", audio_waveform_2_window.shape)
        #print("audio_window_env s ", audio_window_env.shape)
        #print("gen_audio_waveform[aSI:aSI + audio_window_length] s ", gen_audio_waveform[aSI:aSI + audio_window_length].shape)
        
        gen_audio_waveform[aSI:aSI + audio_window_length] += audio_waveform_2_window * audio_window_env

        torchaudio.save(file_name, gen_audio_waveform.unsqueeze(0), audio_orig_sample_rate)
        
        
create_soundstream_audio(audio_all_data[0][0], 60 * mocap_fps, 10 * mocap_fps, "soundstream_audio.wav")

@torch.no_grad()
def create_gen_audio(mocap_data, waveform_data, mocap_start_frame_index, mocap_frame_count, file_name):
    
    transformer.eval()
    
    # prepare mocap data
    mocap_end_frame_index = mocap_start_frame_index + mocap_frame_count
    mocap_data = mocap_data[mocap_start_frame_index:mocap_end_frame_index, ...].to(device)
    mocap_data_norm = (mocap_data - mocap_mean) / (mocap_std + 1e-8)
    mocap_data_norm = mocap_data_norm.unsqueeze(0)
    
    #print("mocap_data_norm s ", mocap_data_norm.shape)
    
    # prepare audio data

    audio_waveform_sample_count = mocap_frame_count * audio_samples_per_mocap_frame
    gen_audio_waveform = torch.zeros((audio_waveform_sample_count), dtype=torch.float32)

    audio_waveform_start_sample_index = mocap_start_frame_index * audio_samples_per_mocap_frame
    audio_waveform_end_sample_index =  audio_waveform_start_sample_index + audio_waveform_input_seq_length
    audio_waveform_data = waveform_data[audio_waveform_start_sample_index:audio_waveform_end_sample_index]
    audio_waveform_data = audio_waveform_data.unsqueeze(0).to(device)
    
    #print("audio_waveform_data s ", audio_waveform_data.shape)
    
    audio_waveform_data = torchaudio.functional.resample(audio_waveform_data, audio_orig_sample_rate, soundstream_audio_sample_rate)
    audio_waveform_data = audio_waveform_data.reshape((1, -1))
    
    #print("audio_waveform_data 2 s ", audio_waveform_data.shape)
    
    audio_spec_data = soundstream.wav_to_spec(audio_waveform_data.to(device))
    
    #print("audio_spec_data s ", audio_spec_data.shape)
    
    audio_spec_data_norm = (audio_spec_data - audio_soundstream_mean) / (audio_soundstream_std + 1e-8)

    #print("audio_spec_data_norm s ", audio_spec_data_norm.shape)
    
    # predict audio
    
    x_mocap = mocap_data_norm[:, :mocap_input_seq_length, ...]
    
    #print("x_mocap s ", x_mocap.shape)
    
    x_audio = audio_spec_data_norm
    
    #print("x_audio s ", x_audio.shape)
    
    yhat_audio = transformer(x_mocap, x_audio)
    
    #print("yhat_audio s ", yhat_audio.shape)

    yhat_audio_spec_norm = yhat_audio.detach().squeeze(0)
    
    #print("yhat_audio_spec_norm s ", yhat_audio_spec_norm.shape)
    
    yhat_audio_spec = yhat_audio_spec_norm * audio_soundstream_std + audio_soundstream_mean

    #print("yhat_audio_spec s ", yhat_audio_spec.shape)
    
    yhat_audio_waveform = soundstream.spec_to_wav(yhat_audio_spec)
    
    #print("yhat_audio_waveform s ", yhat_audio_waveform.shape)
    
    yhat_audio_waveform = yhat_audio_waveform.flatten()
    yhat_audio_waveform = torchaudio.functional.resample(yhat_audio_waveform, soundstream_audio_sample_rate, audio_orig_sample_rate)
    
    #print("yhat_audio_waveform 2 s ", yhat_audio_waveform.shape)
    
    yhat_audio_window = yhat_audio_waveform.reshape(-1)[-audio_window_length:]
    yhat_audio_window = yhat_audio_window.detach().cpu()
    
    #print("yhat_audio_window s ", yhat_audio_window.shape)
    
    gen_audio_waveform[:audio_window_length] += yhat_audio_window * audio_window_env

    for mFI in range(1, mocap_frame_count - mocap_input_seq_length):  
        
        #print("mFI ", mFI)
    
        aSI = mFI * audio_samples_per_mocap_frame
        
        x_mocap = mocap_data_norm[:, mFI:mFI + mocap_input_seq_length, ...]
        x_audio = yhat_audio.detach()
        
        #print("x_mocap s ", x_mocap.shape)
        #print("x_audio s ", x_audio.shape)
        
        yhat_audio = transformer(x_mocap, x_audio)
        
        #print("yhat_audio s ", yhat_audio.shape)
        
        yhat_audio_spec_norm = yhat_audio.detach().squeeze(0)
        
        #print("yhat_audio_spec_norm s ", yhat_audio_spec_norm.shape)
        
        yhat_audio_spec = yhat_audio_spec_norm * audio_soundstream_std + audio_soundstream_mean

        #print("yhat_audio_spec s ", yhat_audio_spec.shape)
        
        yhat_audio_waveform = soundstream.spec_to_wav(yhat_audio_spec)
        
        #print("yhat_audio_waveform s ", yhat_audio_waveform.shape)
        
        yhat_audio_waveform = yhat_audio_waveform.flatten()
        yhat_audio_waveform = torchaudio.functional.resample(yhat_audio_waveform, soundstream_audio_sample_rate, audio_orig_sample_rate)
        
        #print("yhat_audio_waveform 2 s ", yhat_audio_waveform.shape)
        
        yhat_audio_window = yhat_audio_waveform.reshape(-1)[-audio_window_length:]
        yhat_audio_window = yhat_audio_window.detach().cpu()
        
        #print("yhat_audio_window s ", yhat_audio_window.shape)
        
        gen_audio_waveform[aSI:aSI+ audio_window_length] += yhat_audio_window * audio_window_env

    torchaudio.save(file_name, gen_audio_waveform.unsqueeze(0), audio_orig_sample_rate)
    
    transformer.train()
    
"""
generate audio with orig mocap data
"""
    
test_mocap_data = torch.from_numpy(mocap_all_data[0]["motion"]["rot_local"]).to(torch.float32)
test_mocap_data = test_mocap_data.reshape(-1, pose_dim)
test_audio_data = audio_all_data[0][0]

#print("test_mocap_data s ", test_mocap_data.shape)
#print("test_audio_data s ", test_audio_data.shape)

test_mocap_start_times = [100, 200, 300]
test_mocap_duration = 30


for test_mocap_start_time in test_mocap_start_times:
    create_mocap_anim(test_mocap_data, test_mocap_start_time * mocap_fps, test_mocap_duration * mocap_fps, "results/anims/orig_mocap_{}-{}.gif".format(test_mocap_start_time, (test_mocap_start_time + test_mocap_duration)))
    create_orig_audio(test_audio_data, test_mocap_start_time * mocap_fps, test_mocap_duration * mocap_fps, "results/audio/orig_audio_{}-{}.wav".format(test_mocap_start_time, (test_mocap_start_time + test_mocap_duration)))
    create_soundstream_audio(test_audio_data, test_mocap_start_time * mocap_fps, test_mocap_duration * mocap_fps, "results/audio/soundstream_audio_{}-{}.wav".format(test_mocap_start_time, (test_mocap_start_time + test_mocap_duration)))
    create_gen_audio(test_mocap_data, test_audio_data, test_mocap_start_time * mocap_fps, test_mocap_duration * mocap_fps, "results/audio/gen_audio_{}-{}_epoch_{}_orig.wav".format(test_mocap_start_time, (test_mocap_start_time + test_mocap_duration), epochs))


"""
generate audio with alternative mocap data
"""

"""
test_mocap_data_file = "E:/data/mocap/Diane/Solos/ZHdK_10.10.2025/fbx_50hz/trial-003.fbx"
test_mocap_valid_ranges = [0, 19000]
"""

"""
test_mocap_data_file = "E:/data/mocap/Motion2Audio/stocos/fbx_50hz/Take_1_50fps_crop.fbx"
test_mocap_valid_ranges = [[0, 16000]]
"""

"""
test_mocap_data_file = "E:/data/mocap/Eleni/fbx_50hz/Eline_Session-002.fbx"
test_mocap_valid_ranges = [644, 41796]
"""

"""
test_mocap_data_file = "E:/data/mocap/Diane/Solos/ZHdK_12.11.2025/fbx_50hz/RepeatedExcerpt/Diane_Take5.fbx"
test_mocap_valid_ranges = [292, 3306]
"""

test_mocap_data_file = "E:/data/mocap/Eleni/Solos/ZHdK_04.12.2025/fbx_50hz/Eline_Session-001.fbx"
mocap_valid_ranges = [[483, 41635]]

if test_mocap_data_file.endswith(".bvh") or test_mocap_data_file.endswith(".BVH"):
    bvh_data = bvh_tools.load(test_mocap_data_file)
    test_mocap_data = mocap_tools.bvh_to_mocap(bvh_data)
elif test_mocap_data_file.endswith(".fbx") or test_mocap_data_file.endswith(".FBX"):
    fbx_data = fbx_tools.load(test_mocap_data_file)
    test_mocap_data = mocap_tools.fbx_to_mocap(fbx_data)[0] # first skeleton only  
    
test_mocap_data["skeleton"]["offsets"] *= mocap_pos_scale
test_mocap_data["motion"]["pos_local"] *= mocap_pos_scale

# set x and z offset of root joint to zero
test_mocap_data["skeleton"]["offsets"][0, 0] = 0.0 
test_mocap_data["skeleton"]["offsets"][0, 2] = 0.0 

if test_mocap_data_file.endswith(".bvh") or test_mocap_data_file.endswith(".BVH"):
    test_mocap_data["motion"]["rot_local"] = mocap_tools.euler_to_quat_bvh(test_mocap_data["motion"]["rot_local_euler"], test_mocap_data["rot_sequence"])
elif test_mocap_data_file.endswith(".fbx") or test_mocap_data_file.endswith(".FBX"):
    test_mocap_data["motion"]["rot_local"] = mocap_tools.euler_to_quat(test_mocap_data["motion"]["rot_local_euler"], test_mocap_data["rot_sequence"])

test_mocap_data = torch.from_numpy(test_mocap_data["motion"]["rot_local"]).to(torch.float32)
test_mocap_data = test_mocap_data.reshape(-1, pose_dim)

test_mocap_start_times = [100, 200, 300]
test_mocap_duration = 30
    
for test_mocap_start_time in test_mocap_start_times:
    create_mocap_anim(test_mocap_data, test_mocap_start_time * mocap_fps, test_mocap_duration * mocap_fps, "results/anims/test_mocap_{}-{}.gif".format(test_mocap_start_time, (test_mocap_start_time + test_mocap_duration)))
    create_gen_audio(test_mocap_data, test_audio_data, test_mocap_start_time * mocap_fps, test_mocap_duration * mocap_fps, "results/audio/gen_audio_{}-{}_epoch_{}_test.wav".format(test_mocap_start_time, (test_mocap_start_time + test_mocap_duration), epochs))

